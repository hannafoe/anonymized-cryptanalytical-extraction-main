import numpy as np
import scipy
import time
from .global_vars import GlobalConfig
from .utils import getAllWeightsAndBiases, predict_manual_fast, forward, matmul, AcceptableFailure
from .signature_recovery import get_more_crit_pts
# ==========
#  Functions for Sign Recovery from Canales-Martinez
# ==========

def isLinear(func, x_low, x_upp, eps=1e-4, tol=1e-9, debug=False):# tol=1e-10 before
    """
    Determines if a function is linear in a given closed interval.

    Parameters
    ----------
    func
        The function. func takes as input a real number (float) and outputs a
        real number (float).
    x_low : float
        Lower bound of the interval.
    x_upp : float
        Upper bound of the interval.
    eps : float, optional
        "Infinitesimal" variation used when computing slopes and derivatives.
    tol : float, optional
        Tolerance to decide if two real numbers (floats) are "equal".
    debug : bool, optional
        Specifies whether debug information is printed to standard output.

    Returns
    -------
    bool
        True if the function is linear in the given interval, False otherwise.
    """
    
    # Let
    #   x_mid = (x_low + x_upp) / 2,
    #   L_low be the line passing through x_low,
    #   L_upp be the line passing through x_upp,
    #   L_mid_low be the line passing through x_mid on the left, and
    #   L_mid_upp be the line passing through x_mid on the right.
    #
    # To check for linearity, we check the following:
    #    (i) f(x_mid) = (f(x_low) + f(x_upp)) / 2
    #   (ii) L_low = L_upp and L_mid_low = L_low and L_mid_upp = L_upp
    
    if debug:
        print(f"Checking [{x_low}, {x_upp}]")
    
    # Check whether x_low and x_upp are too close
    if (np.abs(x_upp - x_low) < tol):
        if debug:
            print("  Points are too close to each other")
        return None
    
    x_mid = (x_low + x_upp) / 2
    
    # Value of func at x_low, x_upp and x_mid
    y_low = func(x_low)
    y_upp = func(x_upp)
    y_mid = func(x_mid)
    
    # Expected value of func at x_mid
    y_mid_exp = (y_low + y_upp) / 2
    
    if debug:
        print(f"  f({x_low})  = {y_low}")
        print(f"  f({x_upp})  = {y_upp}")
        print(f"  f({x_mid})  = {y_mid}")
        print(f"  fe({x_mid}) = {y_mid_exp}")
        print(f"  |f_mid - fe_mid| = {np.abs(y_mid - y_mid_exp)}")
    
    # Check (i) f(x_mid) = (f(x_low) + f(x_upp)) / 2
    if (np.abs(y_mid - y_mid_exp) > tol):
        return False
    
    # Slopes of L_low and L_upp
    m_low = (func(x_low + eps) - y_low) / eps
    m_upp = (y_upp - func(x_upp - eps)) / eps
    
    if debug:
        print(f"  m_low = {m_low}")
        print(f"  m_upp = {m_upp}")
        print(f"  |m_low - m_upp| = {np.abs(m_low - m_upp)}")
    
    # Check (ii) L_low = L_upp
    if (np.abs(m_low - m_upp) > tol):
        return False
    
    # Slopes of L_mid_low and L_mid_upp
    m_mid_low = (y_mid - func(x_mid - eps)) / eps
    m_mid_upp = (func(x_mid + eps) - y_mid) / eps
    
    if debug:
        print(f"  m_mid_low = {m_mid_low}")
        print(f"  m_mid_upp = {m_mid_upp}")
        print(f"  |m_mid_low - m_low| = {np.abs(m_mid_low - m_low)}")
        print(f"  |m_mid_upp - m_upp| = {np.abs(m_mid_upp - m_upp)}")
    
    # Check (ii) L_mid_low = L_low and L_mid_upp = L_upp
    if (np.abs(m_mid_low - m_low) > tol) or (np.abs(m_mid_upp - m_upp) > tol):
        return False
    
    return True

def getProjection(v, basis):
    '''
    Compute the projection of vector v onto the vector space generated by the
    given orthogonal basis. The vectors of the basis are given as row vectors.

    Parameters
    ----------
    v : array
        1-dimensional array representing the vector v.
    basis : array
        2-dimensional array with the row vectors of the orthogonal basis.

    Returns
    -------
    array
        1-dimensional array representing the projection of v.
    '''
    res = np.zeros_like(v)
    for bi in basis:
        res += (np.dot(v, bi) / np.dot(bi, bi)) * bi
    return res
def getWigglesProjection(weights, signaturesProj, diffs, diffsEps, lyrEps):
    '''
    signaturesProj contains the projections as row vectors
    '''
    # Get wiggles in the layer
    # Case float16
    if signaturesProj.dtype == np.float16:
        signaturesProj_32 = signaturesProj.astype(np.float32)
        norm_32 = np.linalg.norm(signaturesProj_32, axis=1)
        norm = norm_32.astype(np.float16)
    else: # Case float64, float32
        norm = np.linalg.norm(signaturesProj, axis=1)
    wigglesLyr = (lyrEps / norm)[:, np.newaxis] * signaturesProj
    # Get wiggles in the input
    # Case float16
    if diffs.dtype == np.float16:
        diffs_32 = diffs.astype(np.float32)
        wigglesLyr_32 = wigglesLyr.astype(np.float32)
        coeff, _, _, _ = np.linalg.lstsq(diffs_32.T, wigglesLyr_32.T, rcond=None)
        coeff = coeff.astype(np.float16)
    else: # Case float64, float32
        coeff, _, _, _ = np.linalg.lstsq(diffs.T, wigglesLyr.T, rcond=None)
    return diffsEps * coeff.T

def getOrthogonalBasis(V):
    '''
    Compute an orthogonal basis for the space generated by the vectors given
    by the rows of matrix V.

    Parameters
    ----------
    V : array
        2-dimensional array with the row vectors.

    Returns
    -------
    array
        Orthogonal basis given by the rows of the returned matrix.
    '''
    rk = np.linalg.matrix_rank(V)
    q, _ = np.linalg.qr(V.T)
    return q[:,0:rk].T

def findCorner(weights, biases, shape, targetNeurons, numPrecQueries, targetValue=1, tol=1e-6,#tol=1e-6,
               timeout=10, dataset=None):
    """
    Given the weights and biases for a sequence of layers, find an input such
    that targetNeurons in the last layer are close to targetValue, with an error
    vector of norm less than tol.

    Parameters
    ----------
    weights :
        A list of weights for the known layers (each of them a 2D array).
    biases :
        A list of biases for the known layers (each of them a 1D array).
    shape : tuple
        The shape of the input that is returned.
    targetNeurons : list
        The indices of the neurons that need to be fixed.
    targetValue : float
        The value that they should be fixed to.
    tol :
        The allowed norm of the error vector.
    timeout:
        Max number of steps before abandoning the current walk and starting from
        scratch with double timeout
    """
    if dataset is None or dataset == 'mnist' or dataset=='cifar':
        x0 = np.random.uniform(low=-1, high=1, size=shape).flatten()
    #elif dataset=='cifar': # This is how Canales-Martinez ran it but honestly we don't need these dataset points, things run just as well without
    #    x0 = getCIFARtestImage()

    for i in range(timeout):
        numPrecQueries+=1

        # Local matrix at the starting point
        M0, b0 = getLocalMatrixAndBias(weights, biases, x0)
        y0 = np.matmul(x0, M0) + b0

        # Compute the point where the target neurons would be zero assuming linearity never breaks
        # pinv = pseudo inverse
        dx = np.matmul(targetValue - y0[targetNeurons], np.linalg.pinv(M0[:,targetNeurons]))

        x1 = x0 + dx

        # If linearity breaks we can end up with unexpected values, so we halve the shift until
        # we get strictly smaller outputs than before. (Strictly smaller targetValue-y1)
        M1, b1 = getLocalMatrixAndBias(weights, biases, x1)
        y1 = np.matmul(x1, M1) + b1
        while ((targetValue-y0)**2)[targetNeurons].sum() < ((targetValue-y1)**2)[targetNeurons].sum():
            dx /= 2
            x1 = x0 + dx
            M1, b1 = getLocalMatrixAndBias(weights, biases, x1)
            y1 = np.matmul(x1, M1) + b1
        # If the outputs are small enough, we exit the loop
        if ((targetValue-y1)**2)[targetNeurons].sum() < tol**2:
            # Ensure the outputs are all slightly larger than the target
            dy = y1[targetNeurons] - targetValue
            dy[dy > 0] = 0
            dy *= -2
            dx = np.matmul(dy, np.linalg.pinv(M1[:,targetNeurons]))
            return (x1 + dx).reshape(shape),numPrecQueries

        # Otherwise we restart from the new x
        x0 = x1

    # If timeout was reached, we restart from a fresh x0 and double the timeout
    return findCorner(weights, biases, shape, targetNeurons,numPrecQueries, targetValue, tol, 2 * timeout)
def getLocalMatrixAndBias(weights, biases, x0):
    """
    Given the weights and biases up to a certain layer, find the equivalent matrix and bias
    around the vicinity of an input x.

    Parameters
    ----------
    weights:
        A list of weights for the known layers (each of them a 2D array).
    biases:
        A list of biases for the known layers (each of them a 1D array).
    x0:
        A 1D array of inputs (of dimension equal to the second dimension of weights[0])
        OR a 2D array which consists of a vector of inputs (along the first dimension)

    Returns
    -------
    M
        A 2D array representing the local matrix
        OR a 3D array representing one matrix per input (along the first dimension)
    b
        A 1D array representing the local bias vector
        OR a 2D array representing one bias per input (along the first dimension)
    """

    # Special case if x0 is not vectorized
    if len(x0.shape) < 2:
        M, b = getLocalMatrixAndBias(weights, biases, np.array([x0]))
        return M[0], b[0]

    MM = []
    bb = []
    for x0i in x0:
        M = weights[0].copy()
        b = biases[0].copy()
        x = np.matmul(x0i, M) + b
        for layer_id in range(1, len(weights)):
            M_hat = weights[layer_id].copy()
            M_hat[x < 0] = 0 #already applies the ReLU kind of, discounts all weight values <0 like ReLU, only if all in this row are <0 then inactive neuron?
            x = np.matmul(x, M_hat) + biases[layer_id]
            b = np.matmul(b, M_hat) + biases[layer_id]
            M = np.matmul(M, M_hat)

        MM.append(M)
        bb.append(b)

    return np.array(MM), np.array(bb)


def findCorner_quantized(weights, biases, shape, targetNeurons, numPrecQueries, targetValue=1, tol=6e-3,
               timeout=10, dataset=None):
    # This is a very similar quantized version of findCorner()
    x0 = np.random.uniform(low=-1, high=1, size=shape).flatten()

    if weights[0].dtype == np.float16:
        x0=x0.astype(np.float16)
    elif weights[0].dtype == np.float32:
        x0=x0.astype(np.float32)
    for i in range(timeout):
        numPrecQueries+=1
        # Local matrix at the starting point
        M0, b0 = getLocalMatrixAndBias(weights, biases, x0)
        y0 = np.matmul(x0, M0) + b0
        # Compute the point where the target neurons would be zero assuming linearity never breaks
        #pinv = pseudo inverse
        if M0.dtype ==np.float16:
            M0_float32 = M0.astype(np.float32)
            pinv_32 = np.linalg.pinv(M0_float32[:, targetNeurons])
            pinv = pinv_32.astype(np.float16)

            # Perform the operations with the converted arrays
            dx = np.matmul(targetValue - y0[targetNeurons], pinv)
        else:
            dx = np.matmul(targetValue - y0[targetNeurons], np.linalg.pinv(M0[:,targetNeurons]))

        x1 = x0 + dx

        # If linearity breaks we can end up with unexpected values, so we halve the shift until
        # we get strictly smaller outputs than before. (Strictly smaller targetValue-y1)
        M1, b1 = getLocalMatrixAndBias(weights, biases, x1)
        y1 = np.matmul(x1, M1) + b1

        while ((targetValue-y0)**2)[targetNeurons].sum() < ((targetValue-y1)**2)[targetNeurons].sum():
            dx /= 2
            x1 = x0 + dx
            M1, b1 = getLocalMatrixAndBias(weights, biases, x1)
            y1 = np.matmul(x1, M1) + b1
        # If the outputs are small enough, we exit the loop
        if ((targetValue-y1)**2)[targetNeurons].sum() < tol**2:
            # Ensure the outputs are all slightly larger than the target
            dy = y1[targetNeurons] - targetValue
            dy[dy > 0] = 0
            dy *= -2
            if M0.dtype == np.float16:
                M1_float32 = M1.astype(np.float32)
                pinv_32 = np.linalg.pinv(M1_float32[:,targetNeurons])
                pinv = pinv_32.astype(np.float16)
                # Perform the operations with the converted arrays
                dx = np.matmul(dy, pinv)
            else:
                dx = np.matmul(dy, np.linalg.pinv(M1[:,targetNeurons]))
            return (x1 + dx).reshape(shape),numPrecQueries

        # Otherwise we restart from the new x
        x0 = x1

    # If timeout was reached, we restart from a fresh x0 and double the timout
    return findCorner_quantized(weights, biases, shape, targetNeurons, numPrecQueries, targetValue, tol, 2 * timeout)


def getHiddenVector(weights, biases, l, x, relu=False):
    """
    Computes the hidden vector resulting from applying the first l hidden layers
    to the given input x.

    Parameters
    ----------
    weights : array
        List of weights corresponding to the hidden layers. The i-th element in
        the list is a 2-dimensional array with the weights of the incoming
        connections to the neurons in the i-th hidden layer.
    biases : array
        List of biases corresponding to the hidden layers. The i-th element in
        the list is a 1-dimensional array with the biases of the neurons in the
        i-th hidden layer.
    l : int
        Number of hidden layers to consider.
    x : array
        1-dimensional array representing an input to the DNN.
    relu : bool, optional
        Specifies whether to compute the hidden vector before (relu=False) or
        after (relu=True) the ReLU in layer l.

    Returns
    -------
    array
        The hidden vector corresponding to x after applying the first l hidden
        layers.
    """
    y = x
    for i in range(l):
        y = np.matmul(y, weights[i]) + biases[i]
        if (i < (l - 1)) or relu:
            y *= (y > 0)
    return y

def getOrthogonalBasisForInnerLayerSpace(x, weights, biases, l, eps):
    '''
    Given the input x to the DNN, compute an orthogonal basis for the vector
    space generated by the linear neighbourhood of x after the l first layers,
    inclusive.

    Parameters
    ----------
    x : array
        1-dimensional array representing an input to the DNN.
    weights : array
        List of weights corresponding to the hidden layers. The i-th element in
        the list is a 2-dimensional array with the weights of the incoming
        connections to the neurons in the i-th hidden layer.
    biases : array
        List of biases corresponding to the hidden layers. The i-th element in
        the list is a 1-dimensional array with the biases of the neurons in the
        i-th hidden layer.
    l : int
        Number of hidden layers to consider.
    eps : float
        "Infinitesimal" variation used for computing the vector space after the
        first l layers. This value should be small enough so that varying each
        coordinate at the input does not cross the boundary of the linear
        neighbourhood of x.

    Returns
    -------
    B : array
        Orthogonal basis of the vector space after the first l layers. The
        basis is given by the rows of the returned matrix.
    diffs : array
        Let f_{1..l} be the function that evaluates the first l layers of the
        DNN. Also, let y = f_{1..l}(x) and h_i = f_{1..l}(x + eps * e_i), where
        e_i is the i-th canonical vector.
        diffs is a 2-dimensional array representing a matrix where the i-th row
        is the vector h_i - y.
    '''
    n = np.prod(x.shape)
    if l > 0:
        M, b = getLocalMatrixAndBias(weights[0:l], biases[0:l], x)
    else:
        M = np.identity(n, dtype=np.double)
        b = np.zeros_like(x)
    # y = f_{1..l}(x)
    y = np.matmul(x, M) + b
    y *= (y > 0)
    # h_i = f_{1..l}(x + eps * e_i)
    xdx = (eps * np.identity(n, dtype=np.double)) + x
    hidVecs = np.matmul(xdx, M) + b #Very slow for float16 hmmm
    hidVecs *= (hidVecs > 0)
    # Find orthogonal basis for the space generated by all h_i - y
    diffs = hidVecs - y
    B = getOrthogonalBasis(diffs)
    if y.dtype == np.float16:
        diffs = diffs.astype(np.float16)
        B = B.astype(np.float16)
    elif y.dtype == np.float32:
        diffs = diffs.astype(np.float32)
        B = B.astype(np.float32)

    return B, diffs

def getLastLayerOutputMatrixBlackbox(func, weights, biases, inputShape, layerId, dataset, eps, tol):
    out = []
    timeout = 3
    for i in range(weights[-1].shape[1]):
        print("Computing output coefficients for neuron "+str(i+1)+"/"+str(weights[-1].shape[1]))
        for k in range(timeout):
            x = findCorner(weights, biases, (1,)+inputShape, [i], targetValue=-eps, dataset=dataset, tol=tol)
            Ml,_ = getLocalMatrixAndBias(weights, biases, x.flatten())
            dx = Ml[:,i].copy().flatten()
            dx = (dx * eps / (dx@Ml)[i]).reshape((1,)+inputShape)
            Mr,_ = getLocalMatrixAndBias(weights, biases, (x+2*dx).flatten())
            if (np.abs(Ml-Mr) > 1e-10).any():
                if k == timeout-1:
                    print("LINEARITY ERROR: try increasing --eps")
                    exit(-1)
                else:
                    continue
            c = func(x+2*dx) - 2 * func(x+dx) + func(x)
            if (c == 0).any():
                print("PRECISION ERROR(1): Try decreasing --eps")
                exit(-1)
            out.append(c.flatten() / eps)
            break
    return np.array(out)


# ==========
#  Functions for Sign Recovery from Carlini
# ==========

def solve_contractive_sign(A,B,weight,bias,model, dimOfLayer):

    print("Solve the extraction problem for contractive networks")
    weights, biases = getAllWeightsAndBiases(model)
        
    def get_preimage(hidden):
        preimage = hidden
        
        for i,(my_A,my_B) in reversed(list(enumerate(zip(A+[weight], B+[bias])))):
            if i == 0:
                res = scipy.optimize.lsq_linear(my_A.T, preimage-my_B,
                                                bounds=(-np.inf, np.inf))
            else:
                res = scipy.optimize.lsq_linear(my_A.T, preimage-my_B,
                                                bounds=(0, np.inf))
            
            preimage = res.x
        return preimage[np.newaxis,:]

    hidden = np.zeros((dimOfLayer))

    preimage = get_preimage(hidden)

    extended_A,extended_B = A+[weight],B+[bias]
    
    standard_out = predict_manual_fast(preimage,weights,biases)

    signs = []

    for axis in range(len(hidden)):
        h = np.array(hidden)
        h[axis] = 10
        preimage_plus = get_preimage(h)
        h[axis] = -10
        preimage_minus = get_preimage(h)

        print("Confirm preimage")

        if np.any(forward(preimage,extended_A,extended_B) > 1e-5):
            raise AcceptableFailure()
        
        out_plus = predict_manual_fast(preimage_plus,weights,biases)
        out_minus = predict_manual_fast(preimage_minus,weights,biases)

        print(standard_out, out_plus, out_minus)

        inverted_if_small = np.sum(np.abs(out_plus-standard_out))
        not_inverted_if_small = np.sum(np.abs(out_minus-standard_out))

        print("One of these should be small",
              inverted_if_small,
              not_inverted_if_small)

        if inverted_if_small < not_inverted_if_small:
            signs.append(-1)
        else:
            signs.append(1)
    return signs, GlobalConfig.crit_query_count,GlobalConfig.query_count

def is_solution_map(args):
    bounds, extra_tuple = args
    r = []
    for i in range(bounds[0], bounds[1]):
        r.append(is_solution((i, extra_tuple)))
    return r


def is_solution(input_tuple):
    signs, (known_A0, known_B0, LAYER, known_hidden_so_far, K, responses) = input_tuple
    new_signs = np.array([-1 if x == '0' else 1 for x in bin((1<<K)+signs)[3:]])

    if signs%100001 == 0:
        # This isn't cheating, but makes things prettier
        print('tick',signs)
        print("Queries: ",GlobalConfig.query_count)

    guess_A0 = known_A0 * new_signs
    guess_B0 = known_B0 * new_signs
    # We're going to set up a system of equations here
    # The matrix is going to have a bunch of rows (equal to number of equations)
    # each row is of the form
    # [h_0 h_1 h_2 h_3 h_4 ... h_n 1]
    # where h_n is the hidden vector after multiplying by the guessed matrix.
    # and 1 is the weight for the bias term
        
    inputs = matmul(known_hidden_so_far, guess_A0, guess_B0)
    inputs[inputs < 0] = 0

    if responses is None:
        responses = np.ones((inputs.shape[0], 1))
    else:
        inputs = np.concatenate([inputs, np.ones((inputs.shape[0],1))], axis=1)
        pass
        
    solution, res, _, _ = scipy.linalg.lstsq(inputs, responses)

    bias = np.dot(inputs, solution)-responses
    res = np.std(bias)

    #print("Recovered vector", solution.flatten())

    if res > 1e-2:
        return (res, new_signs, solution), 0

    bias = bias.mean(axis=0)

    #solution = np.concatenate([solution, [-bias]])[:, np.newaxis]
    mat = (solution/solution[0][0])[:-1,:]
    if np.any(np.isnan(mat)) or np.any(np.isinf(mat)):
        print("Invalid solution")
        return (res, new_signs, solution), 0
    else:
        s = solution/solution[0][0]
        s[np.abs(s)<1e-14] = 0

        return (res, new_signs, solution), 1

def solve_layer_sign(pool,A,B, known_A0, known_B0, critical_points, LAYER, model, dimInput,dimOfPrevLayer,dimOfLayer,special,
                     already_checked_critical_points=False,
                     only_need_positive=False, l1_mask=None):
    """
    Compute the signs for one layer of the network.
    """
    starttime = time.time()

    critical_points = get_more_crit_pts(A,B, known_A0, known_B0, critical_points, LAYER, model, dimInput,dimOfPrevLayer,dimOfLayer,special,already_checked_critical_points,only_need_positive, l1_mask)
    print("Now have critical points", len(critical_points))
    print("Query count", GlobalConfig.query_count)
    stoptime = time.time()
    tFindCrt = stoptime - starttime

    K = dimOfLayer
    MAX = (1<<K)
    if already_checked_critical_points:
        bounds = [(MAX-1, MAX)]
    else:
        bounds = []
        for i in range(1024):
            bounds.append(((MAX*i)//1024, (MAX*(i+1))//1024))

    print("Created a list")
    
    known_hidden_so_far = forward(critical_points,A,B, with_relu=True)

    start_time = time.time()
    extra_args_tup = (known_A0, known_B0, LAYER, known_hidden_so_far, K, None)
    print("Bounds length: ",len(bounds), len(bounds[0]),len(bounds[1]),len(known_hidden_so_far))
    try:
        all_res = pool.map_async(is_solution_map, [(bound, extra_args_tup) for bound in bounds]).get()
    except KeyboardInterrupt:
        print("Interrupted by user, terminating the pool.")
        pool.terminate()  # Immediately terminate the pool
    finally:
        pool.close()  # Cleanly close the pool resources
        pool.join() 
    #all_res = list(map(is_solution_map, [(bound, extra_args_tup) for bound in bounds]))
    end_time = time.time()

    print("Done map, now collect results")
    print("Took", end_time-start_time, 'seconds')

    all_res = [x for y in all_res for x in y]
    
    scores = [r[0] for r in all_res]
    solution_attempts = sum([r[1] for r in all_res])
    total_attempts = len(all_res)
    
    print("Attempts at solution:", (solution_attempts), 'out of', total_attempts)

    
    std = np.std([x[0] for x in scores])
    print('std',std)
    print('median', np.median([x[0] for x in scores]))
    print('min', np.min([x[0] for x in scores]))
    return min(scores,key=lambda x: x[0])[1], critical_points, GlobalConfig.crit_query_count,GlobalConfig.query_count,tFindCrt